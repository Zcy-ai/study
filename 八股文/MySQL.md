### 索引
索引是数据的目录
* 按「数据结构」分类：B+tree索引、Hash索引、Full-text索引
* 按「物理存储」分类：聚簇索引（主键索引）、二级索引（辅助索引）
* 按「字段特性」分类：主键索引、唯一索引、普通索引、前缀索引
* 按「字段个数」分类：单列索引、联合索引

InnoDB是MYSQL的默认存储引擎，B+Tree索引类型也是MySQL存储引擎采用最多的索引类型

如果有主键，默认会使用主键作为聚簇索引的索引键（key）
如果没有主键，就选择第一个不包含NULL值的唯一列作为聚簇索引的索引键（key）
在上面两个都没有的情况下，InnoDB将自动生成一个隐式自增id列作为聚簇索引的索引键
其它索引都属于辅助索引。创建的主键索引和二级索引默认使用的是B+Tree索引

B+Tree是一种多叉树，叶子节点才存放数据，非叶子节点只存放索引，而且每个节点里的数据是按主键顺序存放的。
每一层父节点的索引值都会出现在下层子节点的索引值中，因此在叶子节点中，包括了所有的索引值信息，并且每一个叶子节点都有两个指针，分别指向下一个叶子节点和上一个叶子节点，形成一个双向链表

B+Tree相比于B树和二叉树来说，最大的优势在于查询效率很高，因为即使数据量很大的情况，查询一个数据的磁盘I/O依然维持在3-4次

主键索引的B+Tree和二级索引的B+Tree：
* 主键索引的B+Tree的叶子节点存放的是实际数据，所有完整的用户记录都存放在主键索引的B+Tree的叶子节点里
* 二级索引的B+Tree的叶子节点存放的是主键值，而不是实际数据

二级索引B+Tree的检索过程：
通过二级索引值找到对应的叶子节点，然后获取主键值，然后再通过主键索引的B+Tree查询到对应的叶子节点，获取数据。这个过程叫做「回表」，也就是说要查两个B+Tree才能查到数据

覆盖索引：二级索引的B+Tree就能查询到结果，比如通过二级索引查主键值

### 为什么MySQL采用B+Tree作为索引
考虑磁盘I/O操作次数，因为MySQL的数据存储在磁盘中
索引的数据结构应满足的要求：
* 能在尽可能少的磁盘的I/O操作中完成查询
* 要能高效地查询某一个记录，也要能高效地执行范围查找

##### 二叉搜索树
二叉搜索树的特点是一个节点的左子树的所有节点都小于这个节点，右子树的所有节点都大于这个节点
缺点：
当每次插入的元素都是二叉查找树中最大的元素，二叉查找树就会退化成了一条链表，查找数据的时间复杂度变成了 O(n)
树的高度就等于每次查询数据时I/O操作的次数

##### 自平衡二叉树
每个节点的左子树和右子树的高度差不能超过 1
缺点：
不管平衡二叉查找树还是红黑树，都会随着插入的元素增多，而导致树的高度变高，这就意味着磁盘 I/O 操作次数多，会影响整体数据查询的效率

##### B树
B 树的每一个节点最多可以包括 M 个子节点，M 称为 B 树的阶，所以 B 树就是一个多叉树
每个节点最多有M-1个数据和最多有M个子节点

缺点：
* B 树的每个节点都包含数据（索引+记录），而用户的记录数据的大小很有可能远远超过了索引数据，这就需要花费更多的磁盘 I/O 操作次数来读到「有用的索引数据」。
* 在我们查询位于底层的某个节点（比如 A 记录）过程中，「非 A 记录节点」里的记录数据会从磁盘加载到内存，但是这些记录数据是没用的，我们只是想读取这些节点的索引数据来做比较查询，而「非 A 记录节点」里的记录数据对我们是没用的，这样不仅增多磁盘 I/O 操作次数，也占用内存资源。
* 如果使用 B 树来做范围查询的话，需要使用中序遍历，这会涉及多个节点的磁盘 I/O问题，从而导致整体速度下降。

##### B+Tree
* 叶子节点（最底部的节点）才会存放实际数据（索引+记录），非叶子节点只会存放索引；
* 所有索引都会在叶子节点出现，叶子节点之间构成一个有序链表；
* 非叶子节点的索引也会同时存在在子节点中，并且是在子节点中所有索引的最大（或最小）
* 非叶子节点中有多少个子节点，就有多少个索引；

1. 单点查询
B+ 树的非叶子节点不存放实际的记录数据，仅存放索引，因此数据量相同的情况下，相比存储即存索引又存记录的 B 树，B+树的非叶子节点可以存放更多的索引，因此 B+ 树可以比 B 树更「矮胖」，查询底层节点的磁盘 I/O次数会更少
2. 插入和删除效率
B+ 树由于存在冗余节点，不会发生复杂的树的变形，它的插入和删除效率更高
3. 范围查询
B+ 树所有叶子节点间还有一个链表进行连接，这种设计对范围查找非常有帮助

B+Tree vs Hash
Hash在做等值查询的时候效率高，复杂度为O（1）


按字段类型分类
主键索引：
一张表最多只有一个主键索引
唯一索引：
UNIQUE，一张表可以有多个唯一索引，索引列的值必须唯一，但是允许有空值
前缀索引：
前缀索引是指对字符类型字段的前几个字符建立的索引
```sql
CREATE TABLE table_name(
    column_list,
    INDEX(column_name(length))
);
```


#### 事务隔离级别如何实现
##### 4个特性：
* 原子性：一个事务中的所有操作，要么全部完成，要么全部不完成。，不会结束在某个中间环节，而且事务在执行过程中发生错误，会被回滚到事务开始前的状态。原子性是通过回滚日志来保证的
* 一致性：事务操作前和操作后，数据满足完整性约束，数据库保持一致性状态。比如，用户 A 和用户 B 在银行分别有 800 元和 600 元，总共 1400 元，用户 A 给用户 B 转账 200 元，分为两个步骤，从 A 的账户扣除 200 元和对 B 的账户增加 200 元。一致性就是要求上述步骤操作后，最后的结果是用户 A 还有 600 元，用户 B 有 800 元，总共 1400 元，而不会出现用户 A 扣除了 200 元，但用户 B 未增加的情况（该情况，用户 A 和 B 均为 600 元，总共 1200 元）。一致性通过持久性、原子性、隔离性来保证
* 隔离性：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致，因为多个事务同时使用相同的数据时，不会相互干扰，每个事务都有一个完整的数据空间，对其他并发事务是隔离的。通过MVCC（多版本并发控制）或锁机制来保证
* 持久性：事务处理结束后，对数据的修改是永久的。通过redo log（重做日志来保证）

##### 脏读（拉完屎不擦）
如果一个事务读到了另一个事务未提交事务修改过的数据，就意味着发生了脏读现象。
🌰：
事务A和事务B是两个并行事务，事务B对数据库中的某条数据做出了更新，但还没有提交事务，此时，事务A读取了该修改过的数据，若事务B发生了回滚，那么事务A刚才得到的数据是过期数据，这种现象称为脏读

##### 不可重复读（读两次就不一样了）
在一个事务内多次读取同一个数据，如果出现前后两次读到的数据不一样的情况，就意味着发生了“不可重复读”现象
🌰：
事务A和事务B是两个并行的事务。事务A先读取了数据库中的一条数据，随后事务B更新了这条数据，那么当事务A再次读取该数据时，就会发生前后两次读到的数据是不一致的现象

##### 幻读（出现了幻觉）
在一个事务内多次查询某个符合查询条件的记录数量，如果出现两次查询到的记录数量不一样的情况，就意味着发生了幻读现象
🌰：
事务A先开始从数据库中查询满足某个条件的记录数量，随后事务B插入了一个符合条件的事务并提交了事务，那么如果事务A再次查询满足该条件的记录数量，就会发现前后两次读到的记录数量不一样，仿佛出现了幻觉一般

##### 事务的隔离级别
* 脏读：读到其他事务未提交的数据
* 不可重复读：前后读取的数据不一致
* 幻读：前后读取记录的数量不一致

##### 四种事务的隔离级别
* 读未提交： 指一个事务还没提交时，它做的变更就能被其他事务看到（仍然可能发生脏读、不可重复读和幻读）
* 读提交：指一个事务提交之后，它做的变更才能被其他事务看到，在每个语句执行之前都会重新生成一个Read View（仍然可能发生不可重复读和幻读）
* 可重复读：指一个事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，MySQL InnoDB引擎的默认隔离级别，启动事务时生成一个Read View（仍然可能发生幻读）
* 串行化： 会对记录加上读写锁，在多个事务对这条记录进行读写操作时，如果发生了读写冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行（三种现象都不会发生）

按隔离水平高低排序：串行化>可重复读>读提交>读未提交

##### Read View在MVCC如何工作
Read View 有四个重要的字段：

* m_ids ：指的是在创建 Read View 时，当前数据库中「活跃事务」的事务 id 列表，注意是一个列表，“活跃事务”指的就是，启动了但还没提交的事务
* min_trx_id ：指的是在创建 Read View 时，当前数据库中「活跃事务」中事务 id 最小的事务，也就是 m_ids 的最小值。
* max_trx_id ：这个并不是 m_ids 的最大值，而是创建 Read View 时当前数据库中应该给下一个事务的 id 值，也就是全局事务中最大的事务 id 值 + 1；
* creator_trx_id ：指的是创建该 Read View 的事务的事务 id。

对于使用 InnoDB 存储引擎的数据库表，它的聚簇索引记录中都包含下面两个隐藏列：
* trx_id，当一个事务对某条聚簇索引记录进行改动时，就会把该事务的事务 id 记录在 trx_id 隐藏列里；
* roll_pointer，每次对某条聚簇索引记录进行改动时，都会把旧版本的记录写入到 undo 日志中，然后这个隐藏列是个指针，指向每一个旧版本记录，于是就可以通过它找到修改前的记录。

一个事务去访问记录的时候，除了自己的更新记录总是可见之外，还有这几种情况：
* 如果记录的 trx_id 值小于 Read View 中的 min_trx_id 值，表示这个版本的记录是在创建 Read View 前已经提交的事务生成的，所以该版本的记录对当前事务可见。
* 如果记录的 trx_id 值大于等于 Read View 中的 max_trx_id 值，表示这个版本的记录是在创建 Read View 后才启动的事务生成的，所以该版本的记录对当前事务不可见。
* 如果记录的 trx_id 值在 Read View 的 min_trx_id 和 max_trx_id 之间，需要判断 trx_id 是否在 m_ids 列表中：
    ** 如果记录的 trx_id 在 m_ids 列表中，表示生成该版本记录的活跃事务依然活跃着（还没提交事务），所以该版本的记录对当前事务不可见。
    ** 如果记录的 trx_id 不在 m_ids列表中，表示生成该版本记录的活跃事务已经被提交，所以该版本的记录对当前事务可见。

这种通过「版本链」来控制并发事务访问同一个记录时的行为就叫 MVCC（多版本并发控制）。

##### 可重复读是如何工作的

##### 读提交是如何工作的
区别主要在于min_trx_id,读提交由于每次语句执行前创建read view，在别人的修改事务提交之后，读提交的事务的min_trx_id也随之更新(变大)，可以读取min_trx_id之前的记录值

###### 为什么可重复读隔离级别仍然会发生幻读现象

MySQL InnoDB引擎的默认隔离级别虽然是可重复读，但是它很大程度上避免了幻读现象

解决方案：
* 针对快照读（普通SELECT语句），是通过MVCC方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入一条数据，是查询不出来这条数据的，所以就很好的避免了幻读问题
* 针对（SELECT...FOR UPDATE等语句），是通过next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行select...for update语句的时候，会加上next-key lock，如果有其他事务在next-key lock锁范围内插入一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好避免了幻读问题

###### 快照读如何避免幻读
可重复读隔离级是由MVCC（多版本并发控制）实现的，实现的方式是开始事务后（执行begin语句后），在执行第一个查询语句后，会创建一个Read View，后续的查询语句利用这个Read View，通过这个Read View就可以在undo log版本链找到事务开始时的数据，所以在事务过程中每次查询的数据都是一样的

###### 当前读如何避免幻读
MySQL里除了普通查询是快照读，其他都是当前读，比如update、insert、delete，这些语句执行前都会查询最新版本的数据，然后再做进一步的操作

Innodb引擎为了解决可重复读隔离级别使用当前读而造成的幻读问题，就引出了间隙锁，假设表中有一个id范围为（3，5）的间隙锁，就无法插入id=4这条记录了，这样就有效防止幻读现象的发生

🌰：
事务A执行了下面的sql语句
```sql
begin;
SELECT name FROM t_stu
WHERE id > 2
FOR UPDATE;
```
事务A执行了这条SQL语句之后，就在对表中的记录加上id范围为(2, +∞] 的 next-key lock（next-key lock 是间隙锁+记录锁的组合）
然后，事务B在执行插入语句的时候，判断到插入的位置被事务A家了next-key lock，于是事务B会生成一个插入意向锁，同时进入等待状态，直到事务A执行commit

###### 仍然发生幻读的场景
🌰：
事务A
```sql
begin;
select * from t_stu where id = 5;
```

事务B
```sql
begin;
insert into t_stu values(5, 'Messi', 35);
commit;
```

事务A
```sql
update t_stu set name = '小林coding' where id = 5;
select * from t_stu where id = 5;
```

在可重复读隔离级别下，事务A第一次执行普通的select语句时生成了一个 ReadView，之后事务B向表中新插入了一条id=5的记录并提交。接着，事务 A对id=5这条记录进行了更新操作，在这个时刻，这条新记录的 trx_id 隐藏列的值就变成了事务A的事务id，之后事务A再使用普通 select 语句去查询这条记录时就可以看到这条记录了，于是就发生了幻读。

trx_id：每个事务在执行时都会被分配一个唯一的trx_id。这个trx_id可以用于在系统日志中追踪事务的执行情况，也可以用于实现MVCC等并发控制机制。需要注意的是，trx_id是在事务开始时分配的，因此如果一个事务中途崩溃了，其trx_id可能会被重用。这种情况下，事务的后续操作可能会被误认为是先前事务的操作。因此，在使用trx_id进行并发控制时，需要注意避免这种情况的发生。

除了上面这一种场景会发生幻读现象之外，还有下面这个场景也会发生幻读现象。
🌰：
T1 时刻：事务 A 先执行「快照读语句」：select * from t_test where id > 100 得到了 3 条记录。
T2 时刻：事务 B 往插入一个 id= 200 的记录并提交；
T3 时刻：事务 A 再执行「当前读语句」 select * from t_test where id > 100 for update 就会得到 4 条记录，此时也发生了幻读现象。
要避免这类特殊场景下发生幻读的现象的话，就是尽量在开启事务之后，马上执行 select ... for update 这类当前读的语句，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录。

##### MySQL日志

* undo log(回滚日志): Innodb存储引擎层生成的日志，实现了事务的原子性，主要用于事务回滚和MVCC
* redo log(重做日志): 是Innodb存储引擎层生成的日志，实现了事务的持久性，主要用于掉电等故障的恢复
* binlog（归档日志): 是Serveur层生成的日志，主要用于数据备份和主从复制

###### undo log
我们在执行一条增删改语句的时候，MySQL会隐式开启事务来执行增删改语句，执行完自动提交事务

插入一条记录：把这条记录的主键记下来，这样之后回滚时只需要把这个主键值对应的记录删掉就
删除一条记录：要把这条记录的内容都记下来，这样之后回滚时再把由这些内容组成的记录插入到表中
更新一条记录：要把被更新的记录的旧值记下来，这样之后回滚时再把这些列更新为旧值

一条记录的每一次更新操作产生的undo log格式都有一个roll_pointer指针和一个trix_id事务id：
* trx_id可以知道该记录是被哪个事务修改的
* roll_pointer指针可以将这些undo log串成一个版本链

undo log最重要的作用就是通过ReadView+undo log实现MVCC（多版本并发控制）
* 读提交隔离级别是在每个select都会生成一个新的Read View，这意味在事务多次读取同一个数据时，可能由于有更新该数据的事务完成提交而导致两次读取的数据值不一致
* 可重复读隔离级别时启动事务时生成一个Read View，然后整个事务期间都在用这个Read View，这样就保证了在事务期间读取到的数据都是事务启动前的数据

###### Buffer Pool
MySQL中的数据都是存储在磁盘中的，那么我们要更新一条记录时，得先从磁盘读取该记录，然后在内存中修改这条记录，修改完的记录会缓存起来

读取数据：如果缓存命中，客户端就会直接读取buffer pool中的数据
修改数据：如果缓存命中，直接修改buffer pool中数据存在的页，然后将其页设置为脏页（表示内存和磁盘上的数据已经不一致），后续由后台线程选择一个合适的时机将脏页写入磁盘（WAL）

###### redo log
对XXX表空间中的YYY数据页ZZZ偏移量的地方做了AAA更新，每当执行一个事务就会产生这样的一条或多条物理日志
在事务提交时，只要先将 redo log 持久化到磁盘即可，可以不需要等到将缓存在 Buffer Pool 里的脏页数据持久化到磁盘。
当系统崩溃时，虽然脏页数据没有持久化，但是 redo log 已经持久化，接着 MySQL 重启后，可以根据 redo log 的内容，将所有数据恢复到最新的状态。

为了防止断电数据的问题，当有一条记录需要更新的时候，InnoDB引擎就会先更新内存（同时标记为脏页），然后将本次对这个页的修改以redo log的形式记录下来，这个时候更新就算完成了
后续由后台线程选择一个合适的时机将缓存在Buffer Pool的脏页刷新到磁盘里，它就是WAL技术
WAL技术指的是，MySQL的写操作并不是立刻写到磁盘上，而是先写日志，然后在合适的时间再写到磁盘上

redo log保证了事务的持久性
###### redo log和 undo log的区别
redo log记录了此次事务完成后的数据状态，记录的是更新之后的值；
undo log记录了此次事务开始前的数据状态，记录的是更新之前的值；

redo log+WAL，可以保证即使数据库发生异常重启，之前已提交的记录都不会丢失-------->crash-safe（崩溃恢复）

###### redo log要写到磁盘，数据也要写到磁盘，为什么要多此一举
写入redo log的方式使用了追加操作，所以磁盘操作是顺序写
写入数据需要先找到写入位置，所以磁盘操作时随机写

顺序写>>随机写

WAL将MySQL的写操作从磁盘的随机写变成了顺序写，提升了语句的执行性能

###### 产生的 redo log 是直接写入磁盘的吗？

不是的。
实际上，执行一个事务的过程中，产生的 redo log 也不是直接写入磁盘的，因为这样会产生大量的 I/O 操作，而且磁盘的运行速度远慢于内存。所以，redo log 也有自己的缓存—— redo log buffer，每当产生一条 redo log 时，会先写入到 redo log buffer

###### redo log什么时候刷盘
* MySQL 正常关闭时；
* 当 redo log buffer 中记录的写入量大于 redo log buffer 内存空间的一半时，会触发落盘；
* InnoDB 的后台线程每隔 1 秒，将 redo log buffer 持久化到磁盘。
* 每次事务提交时都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘（这个策略可由 innodb_flush_log_at_trx_commit 参数控制，下面会说）。
* innodb_flush_at_trx_commit参数
    ** 当设置该参数为 0 时，表示每次事务提交时 ，还是将 redo log 留在 redo log buffer 中 ，该模式下在事务提交时不会主动触发写入磁盘的操作。
    ** 当设置该参数为 1 时，表示每次事务提交时，都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘，这样可以保证 MySQL 异常重启之后数据不会丢失。
    ** 当设置该参数为 2 时，表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log 写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘，因为操作系统的文件系统中有个 Page Cache

###### redo log文件写满了怎么办
redo log 文件组是以循环写的方式工作的，从头开始写，写到末尾就回到开头
redo log 是循环写的方式，相当于一个环形，InnoDB 用 write pos 表示 redo log 当前记录写到的位置，用 checkpoint 表示当前要擦除的位置。如果 write pos 追上了 checkpoint，就意味着 redo log 文件满了，这时 MySQL 不能再执行新的更新操作，也就是说 MySQL 会被阻塞（因此所以针对并发量大的系统，适当设置 redo log 的文件大小非常重要），此时会停下来将 Buffer Pool 中的脏页刷新到磁盘中，然后标记 redo log 哪些记录可以被擦除，接着对旧的 redo log 记录进行擦除，等擦除完旧记录腾出了空间，checkpoint 就会往后移动（图中顺时针），然后 MySQL 恢复正常运行，继续执行新的更新操作。

###### binlog
binlog是MySQL的Serveur层实现的日志
binlog是追加写，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志，保存全量的日志
binlog用于备份恢复、主从复制
MySQL 在完成一条更新操作后，Server 层还会生成一条 binlog，等之后事务提交的时候，会将该事物执行过程中产生的所有 binlog 统一写 入 binlog 文件
三种数据格式：STATEMENT、ROW、MIXED

如果不小心数据库的数据被删除了，能使用redo log文件恢复数据吗？
只能使用binlog文件恢复，因为redo log是循环写，是会边写边擦除日志的，只记录未被刷入磁盘的数据的物理日志，已经刷入磁盘的数据都会被redo log擦除，binlog保存全量的日志，也就是保存了所有数据变更的情况，理论上记录在binlog上的数据，都可以恢复

###### binlog什么时候刷盘
事务执行过程中，先把日志写到binlog cache（Serveur层的cache），事务提交的时候，再把binlog cache写到binlog文件中。


###### 主从复制
MySQL的主从复制依赖于binlog，也就是记录MySQL上的所有变化并以二进制形式保存在磁盘上。复制的过程就是将binlog的数据从主库传输到从库上。
这个过程一般是异步的，也就是主库上执行事务操作的线程不会等待复制binlog的线程同步完成

MySQL主从复制的三个阶段：写入binlog、同步binlog、回放binlog
* MySQL 主库在收到客户端提交事务的请求之后，会先写入 binlog，再提交事务，更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应。
* 从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库“复制成功”的响应。
* 从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性。

从库是不是越多越好？
从库数量增加，从库连接的I/O线程也比较多，主库也要创建同样多的log dump线程来处理复制的请求，对主库资源消耗比较高，同时还受限于主库的网络带宽

MySQL主从复制模型
* MySQL主库提交事务的线程要等待所有从库的复制成功响应，才返回客户端结果。基本没法用
* 异步复制（默认模型）：MySQL主库提交事务的线程并不会等待binlog同步到各从库，就返回客户端结果。这种模式一旦主库宕机，数据就会发生丢失
* 半同步复制：事务线程不用等待所有的从库复制成功响应，只要一部分复制成功响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回客户端。

###### 主从提交
事务提交后，redo log和binlog都要持久化到磁盘，但是这两个是独立的逻辑，可能出现半成功的状态，这样就可能造成两份日志之间的逻辑不一致

🌰：
假设次我们对一行数据作出更新
* 如果在将redo log刷入到磁盘之后，MySQL突然宕机了，而binlog还没有来得及写入。MySQL重启后，通过redo log能将buffer pool中的这行数据恢复到新值，但是binlog里面没有这条记录的更新语句，在主从架构中，binlog会被复制到从库，由于binlog丢失了这条更新语句，从库的这一行数据仍然是旧值，与主库不一致
* 如果在将binlog刷入到磁盘之后，MySQL突然宕机了，而redo log还没有来得及写入。由于redo log还没写，崩溃恢复之后这个事务无效，所以这行数据还是旧值，而binlog里面记录了这条更新语句，在主从架构中，binlog会被复制到从库，从库的这行数据将与主库不一致

MySQL为了避免redo log和bin log之间的逻辑不一样的问题，使用了两阶段提交来解决
两阶段提交把单个事务的提交拆分成了2个阶段，分别是准备阶段和提交阶段

当客户端执行commit语句或者在自动提交的情况下，MySQL内部开启一个XA事务，分两个阶段来完成XA事务的提交
XA事务执行步骤：
* prepare阶段：将XID（内部XA事务的ID）写入到redo log，同时将redo log对应的事务状态设置为prepare，然后将redo log持久化到磁盘（innodb_flush_log_at_trx_commit=1的作用）
* commit阶段：将XID写入到binlog，然后将binlog持久化到磁盘（sync_binlog=1的作用），接着调用引擎的提交事务接口，将redo log状态设置为commit此时该状态并不需要持久化到磁盘，只需要write到文件系统的page cache中就够了，因为只要binlog写磁盘成功，就算redo log的状态还可以是prepare也没有关系，一样会被认为事务已经执行成功

时刻A：redo log写入磁盘，binlog还没写入磁盘的时刻
时刻B：redo log和binlog都写入磁盘的时刻

无论时刻A还是时刻B，redolog都处于prepare状态
* 如果binlog中没有当前内部XA事务的XID，说明redolog完成刷盘，但是binlog还没有刷盘，则回滚事务。对应时刻A崩溃恢复的情况
* 如果binlog中有当前内部XA事务的XID，说明redolog和binlog都完成了刷盘，则提交事务。对应时刻B崩溃恢复的情况


事务还没提交的时候，redolog也会被持久到磁盘中吗
事务执行中间过程的redo log也是直接卸载redo log buffer中的，这些缓存在redo log buffer里的redo log也会被后台线程每隔一秒一起持久到磁盘
redo log可以在事务没提交之前持久化到磁盘，但是binlog必须在事务提交之后，才可以持久化到磁盘

两阶段提交的弊端：
* 磁盘I/O次数高：每个事务提交都会进行两次fsync（刷盘），一次是redo log刷盘，另一次是binlog刷盘
* 两阶段提交虽然能够保证单个事务两个日志的内容一致，但在多事务的情况下，却不能保证两者的提交顺序一致，因此，在两阶段提交的流程基础上，还需要加一个🔒来保证提交的原子性，从而保证多事务的情况下，两个日志的提交顺序一致


###### 组提交
MySQL 引入了 binlog 组提交（group commit）机制，当有多个事务提交的时候，会将多个 binlog 刷盘操作合并成一个，从而减少磁盘 I/O 的次数，如果说 10 个事务依次排队刷盘的时间成本是 10，那么将这 10 个事务一次性一起刷盘的时间成本则近似于 1

引入了组提交机制后，prepare 阶段不变，只针对 commit 阶段，将 commit 阶段拆分为三个过程：
* flush 阶段：多个事务按进入的顺序将 binlog 从 cache 写入文件（不刷盘）；
* sync 阶段：对 binlog 文件做 fsync 操作（多个事务的 binlog 合并一次刷盘）；
* commit 阶段：各个事务按顺序做 InnoDB commit 操作；

上面的每个阶段都有一个队列，每个阶段有锁进行保护，因此保证了事务写入的顺序，第一个进入队列的事务会成为 leader，leader领导所在队列的所有事务，全权负责整队的操作，完成后通知队内其他事务操作结束。对每个阶段引入了队列后，锁就只针对每个队列进行保护，不再锁住提交事务的整个过程，可以看的出来，锁粒度减小了，这样就使得多个阶段可以并发执行，从而提升效率。

